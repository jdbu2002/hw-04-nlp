{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146b4b25",
   "metadata": {},
   "source": [
    "# HW04 - NLP\n",
    "## Punto III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5423426",
   "metadata": {},
   "source": [
    "You are going to build a classifier to identify the most likely author for a set of input lines of text (I suggest utilizing text segments comprising 150 to 250 words). It is a multinomial classification task (3 classes).\n",
    "- Describe how you prepare the dataset. Create the training, validation, and testing sets. Make a summary table with the dimensions (number of samples) by class for each one of the previous data sets.\n",
    "- Define three feed-forward (dense) neural network architectures in Keras that make use of the previously built embeddings.\n",
    "    - Explain the dimensions of each layer of each architecture (model summary).\n",
    "- Describe the results of combining the 3 architectures with the 3 types of embeddings in terms of accuracy, precision and recall in tests set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "942b1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c5b86",
   "metadata": {},
   "source": [
    "Reutilización de código del punto 1 para procesar los textos. Se agrega la función create_text_samples para generar muestras de texto de longitud variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbc7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_text(f) -> list[str]:\n",
    "    begun = False\n",
    "    full_text = []\n",
    "    paragraph = \"\"\n",
    "\n",
    "    for base_line in f:\n",
    "        line = base_line.strip()\n",
    "\n",
    "        if len(line) == 0:\n",
    "            if len(paragraph) > 0:\n",
    "                full_text.append(paragraph.strip())\n",
    "                paragraph = \"\"\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"*** START OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "            begun = True\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"*** END OF THE PROJECT GUTENBERG EBOOK\"):\n",
    "            break\n",
    "\n",
    "        if begun:\n",
    "            paragraph += line + \" \"\n",
    "\n",
    "    return full_text\n",
    "\n",
    "def create_text_samples(text, min_words=150, max_words=250) -> list[str]:\n",
    "    \"\"\"Crea segmentos de texto entre min_words y max_words palabras\n",
    "    input: text (str): texto completo\n",
    "           min_words (int): mínimo de palabras por muestra\n",
    "           max_words (int): máximo de palabras por muestra\n",
    "    output: list of str: lista de muestras de texto\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    words = text.split()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Tomar un segmento aleatorio entre min_words y max_words\n",
    "        sample_size = np.random.randint(min_words, max_words + 1)\n",
    "        if i + sample_size <= len(words):\n",
    "            sample = ' '.join(words[i:i+sample_size])\n",
    "            samples.append(sample)\n",
    "            i += sample_size\n",
    "        else:\n",
    "            # Último segmento si queda texto\n",
    "            if len(words) - i >= min_words:\n",
    "                sample = ' '.join(words[i:])\n",
    "                samples.append(sample)\n",
    "            break\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c02367",
   "metadata": {},
   "source": [
    "Cargamos los datos y preprocesamos los textos usando las funciones definidas anteriormente. Se asocia cada texto con su autor y libro correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b65561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./books\"\n",
    "books = os.listdir(base_path)\n",
    "\n",
    "data = []\n",
    "author_mapping = {\n",
    "    'arthur': 'Arthur Conan Doyle',\n",
    "    'lewis': 'Lewis Carroll',\n",
    "    'shakespear': 'William Shakespeare'\n",
    "}\n",
    "\n",
    "for book in books:\n",
    "    # Identificar autor\n",
    "    author_key = book.split('-')[0]\n",
    "    author = author_mapping[author_key]\n",
    "\n",
    "    path = os.path.join(base_path, book)\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        paragraphs = serialize_text(f)\n",
    "        full_text = ' '.join(paragraphs)\n",
    "\n",
    "        # Crear muestras de 150-250 palabras\n",
    "        samples = create_text_samples(full_text, min_words=150, max_words=250)\n",
    "\n",
    "        for sample in samples:\n",
    "            data.append({\n",
    "                'text': sample,\n",
    "                'author': author,\n",
    "                'book': book.replace('.txt', '')\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785698",
   "metadata": {},
   "source": [
    "Creamos el DataFrame con las muestras generadas, mapeamos los autores a IDs numéricos, y dividimos el dataset en conjuntos de entrenamiento, validación y prueba. Finalmente, generamos una tabla resumen con la distribución de muestras por autor en cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5bd6241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de muestras: 1795\n",
      "\n",
      "Distribución por autor:\n",
      "author\n",
      "Arthur Conan Doyle     1073\n",
      "William Shakespeare     412\n",
      "Lewis Carroll           310\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución por libro:\n",
      "book\n",
      "arthur-return-sherlock      560\n",
      "arthur-hound-baskerville    298\n",
      "arthur-the-sign-of-four     215\n",
      "shakespear-hamlet           158\n",
      "lewis-glass                 148\n",
      "shakespear-king-henry       136\n",
      "lewis-alice-wonderland      134\n",
      "shakespear-the-temptest     118\n",
      "lewis-hunting                28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== DATASET SPLITS ===\n",
      "Train: 1256 samples\n",
      "Validation: 269 samples\n",
      "Test: 270 samples\n",
      "\n",
      "=== SUMMARY TABLE ===\n",
      "Dataset              Test  Train  Validation  Total\n",
      "Author                                             \n",
      "Arthur Conan Doyle    161    751         161   1073\n",
      "Lewis Carroll          47    217          46    310\n",
      "William Shakespeare    62    288          62    412\n",
      "\n",
      "Total samples: 1795\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapear autores a números\n",
    "author_to_id = {author: idx for idx, author in enumerate(df['author'].unique())}\n",
    "df['author_id'] = df['author'].map(author_to_id)\n",
    "\n",
    "print(f\"Total de muestras: {len(df)}\")\n",
    "print(f\"\\nDistribución por autor:\")\n",
    "print(df['author'].value_counts())\n",
    "print(f\"\\nDistribución por libro:\")\n",
    "print(df['book'].value_counts())\n",
    "\n",
    "# Dividir en train, validation, test (70%, 15%, 15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['author_id'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['author_id'], random_state=42)\n",
    "\n",
    "print(f\"\\n=== DATASET SPLITS ===\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Crear tabla resumen\n",
    "summary_data = []\n",
    "for dataset_name, dataset in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    for author in df['author'].unique():\n",
    "        count = len(dataset[dataset['author'] == author])\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Author': author,\n",
    "            'Samples': count\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_pivot = summary_df.pivot(index='Author', columns='Dataset', values='Samples')\n",
    "summary_pivot['Total'] = summary_pivot.sum(axis=1)\n",
    "\n",
    "print(\"\\n=== SUMMARY TABLE ===\")\n",
    "print(summary_pivot)\n",
    "print(f\"\\nTotal samples: {summary_pivot['Total'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dea56",
   "metadata": {},
   "source": [
    "Reutilizamos el mismo preprocesamiento de textos del punto 1, con los cuales se generaron los embeddings pre-entrenados. Este paso es necesario antes de usar el tokenizador de Keras. Si no se hace este preprocesamiento, los embeddings pre-entrenados no serán efectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292f2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    processed = text.lower()  # Solo minúsculas\n",
    "    processed = re.sub(r'[^a-z\\s\\']', ' ', processed)  # Mantener letras y apóstrofes\n",
    "    processed = re.sub(r'\\s+', ' ', processed).strip()  # Normalizar espacios\n",
    "    tokens = processed.split()\n",
    "    tokens = [token for token in tokens if len(token) > 1]  # Eliminar tokens de 1 letra\n",
    "    return tokens \n",
    "\n",
    "train_df['text_processed'] = train_df['text'].apply(tokenize)\n",
    "val_df['text_processed'] = val_df['text'].apply(tokenize)\n",
    "test_df['text_processed'] = test_df['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ce5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hw04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
